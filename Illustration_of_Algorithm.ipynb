{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project members:\n",
    "Mengshu Shao, Yelena Kernogitski\n",
    "\n",
    "### GitHub Repositories: \n",
    "https://github.com/yelenakernogitski/Spectral-Clustering-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Clustering is one of the building blocks in modern data analysis, and has been widely used in machine learning and pattern recognition. Two commonly used approaches are K-means and learning a mixture model using EM. However, these methods have some drawbacks, such as violation of the harsh simplifying assumption which the density of each cluster is Gaussian. One method that provides a possible solution in finding useful clusters is spectral clustering, which utilizes eigenvectors derived from the distance between points. The method performs dimensionality reduction in order to find clusters. In this report, First we implement a simple spectral clustering algorithm for clustering points in . Second we analyze how it works in “ideal” case in which the points are exactly far apart (i.e., when affinity matrix s strictly block diagonal), and in general case in which affinity’s off-diagonal blocks are non-zero. Then we test the algorithm by applying it to a number of challenging clustering problems. Further, we attempt to optimize the algorithm using within-Python options (such as vectorization) in addition to JIT and Cython wrapping functions. Finally, we compare the original method in Python and the latter, higher performance method by determining the efficiency of each method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant aim of machine learning and pattern recognition is finding appropriate clusters that are able to accurately classify data points. Two standard approaches used in clustering are K-means and Expectation-Maximazation which allow for learning a mixture model. The latter has severe drawbacks where we must estimate the density of each cluster to be Gaussian due to parametric assumptions, and the log likelihood may result in getting caught in a local minima, and many various initial values may need to be used [1]. K-means clustering by itself leads to other problems, such as not being able to accomodate non-spherical data, and other complex datasets. A method that provides a possible solution in finding useful clusters in spectral clustering, which utilizes eigenvectors derived from the distance between points. An additional part of our algorithm uses the eigenvectors found in K-means clustering, as opposed to using the original dataset found. It has been found in empirical studies that the latter method is more accurate in classifying clusters [1]. In this report, we first implement a simple spectral clustering algorithm for clustering point in $\\mathbb{R}^n$. Second, we analyze how it works in an \"ideal\" case in which the points are exactly far apart. Then we apply the algorithm to a number of challenging clustering problems. We will attempt to optimize our algorithm by using various methods--such as vectorization, JIT, and cython wrapping funcions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm has been previously described in (Ng et al, 2002). Provided a set of points $S= {s_1,...,s_n}$ in $\\mathbb{R}^l$ that we want to clsuter into $k$ subsets:\n",
    "\n",
<<<<<<< HEAD
    "1. Define the affinity matrix $A \\in \\mathbb{R}^{nxn}$ by $A_{ij}=exp(-||s_i-s_j||^2 / 2\\sigma^2)$ if $i\\neq{j}$, and $A_{ii}=0$.\n",
=======
    "1. Define the affinity matrix $A$ $\\mathbb{R}^{nxn}$ by $A_{ij}=exp(-||s_i-s_j||^2 / 2\\sigma^2)$ if $i\\neq{j}$, and $A_{ii}=0$.\n",
>>>>>>> 26c0d7d12ea6d0d7e3e096f7713fb9c87102d9fb
    "    \n",
    "2. Create a diagonal matrix whose ($i$,$i$) element is the sum of $A's$ i-th row, and construct L, where $L= D^{-1/2}AD^{-1/2}$.\n",
    "    \n",
    "3. Find ${x_1,...,x_n}$, the $k$ largest eigenvectors of $L$, and create the matrix $X$, by placing the k eigenvectors in columns.\n",
    "    \n",
    "4. Use $X$ to form the matrix $Y$, by normalizing each X's rows to have unit length (i.e. $Y_{ij}=X_{ij}/(\\sum_{j}{}X_{ij}^2)^{1/2}$). \n",
    "    \n",
    "5. Treating each row of $Y$ as a point in $\\mathbb{R}^k$, cluster into $k$ clusters using K-means\n",
    "    \n",
    "6. Assign the original point $s_i$ to cluster $j$ if row $i$ of the matrix $Y$ was assigned to cluster $j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our scaling parameter $\\sigma^2$ was found by minimizing the distortion of the Y matrix and the clustering centers that correspond to each assigned cluster. The distortion was found by summing over the Euclidean distances between each data point and the center of its assigned cluster. We found $\\sigma^2$ by using an iterative procedure to find which value of $\\sigma^2$ minimized the distortion. Generally, $\\sigma^2$ can be determined using human input, but this process automizes the procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from numpy.core.umath_tests import inner1d\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from more_itertools import unique_everseen\n",
    "import numba\n",
    "from numba import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we'll illustrate the algorithm step by step on a simple matrix.\n",
    "\n",
<<<<<<< HEAD
    "Our simple example: given a set of points $S = \\{s_1, s_2, s_3\\}$ in $\\mathbb{R}^2$ that we want to cluster into $k$ subsets. Here $k$ shoulb be less than or equal to the number of points. The reason of using this simple matrix is to simply and directly understand what each step attempts to do."
=======
    "Our simple example: given a set of points $S = \\{s_1, s_2, s_3\\}$ in $\\mathbb{R}^2$ that we want to cluster into $k$ subsets. Here $k$ shoulb be less than or equal to the number of points."
>>>>>>> 26c0d7d12ea6d0d7e3e096f7713fb9c87102d9fb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [3, 4],\n",
       "       [5, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array([[2,1], [3,4], [5,4]])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form the affinity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the affinity matrix $A \\in \\mathbb{R}^{nxn}$, defined by $A_{ij}=exp(-||s_i-s_j||^2 / 2\\sigma^2)$ if $i\\neq{j}$, and $A_{ii}=0$.\n",
    "\n",
    "$\\textbf{affinity}(s, var)$\n",
    "* Form the affinity matrix\n",
    "* Parameters\n",
    "    * s: array-like, shape (n_samples, n_features)\n",
    "        * Training vectors, where n_samples is the number of samples and n_features is the number of features.\n",
    "    * var: float, $\\sigma^2$\n",
    "        * scaling parameter controlling how rapidly the affinity $A_{ij}$ falls off with the distance between $s_i$ and $s_j$.\n",
    "* Returns\n",
    "    * A: array-like, shape (n_samples, n_samples)\n",
    "        * Affinity matrix.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   6.73794700e-03,   1.23409804e-04],\n",
       "       [  6.73794700e-03,   0.00000000e+00,   1.35335283e-01],\n",
       "       [  1.23409804e-04,   1.35335283e-01,   1.00000000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def affinity(s, var):\n",
    "    n = np.shape(s)[0]\n",
    "    A = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            A[i, i] = 0\n",
    "            A[i,j] = np.exp(-(la.norm(s[i] - s[j])**2) / (2*var))\n",
    "    return A\n",
    "\n",
    "A=affinity(s,1)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define D and L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $D$ as a diagonal matrix whose ($i$,$i$) element is the sum of $A's$ $i-th$ row, and construct $L$, where $L= D^{-1/2}AD^{-1/2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 145.74376886,    0.        ,    0.        ],\n",
       "       [   0.        ,    7.03862366,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.88070135]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.shape(s)[0]\n",
    "D = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    D[i, i] = 1 / (A[i].sum())\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.21580746,  0.00139817],\n",
       "       [ 0.21580746,  0.        ,  0.33695293],\n",
       "       [ 0.00139817,  0.33695293,  0.88070135]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = np.sqrt(D).dot(A).dot(np.sqrt(D))\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the k largest eigenvectors of L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find ${x_1,...,x_n}$, the $k$ largest eigenvectors of $L$, and create the matrix $X$, by placing the k eigenvectors in columns. Here we set $k=2$ clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the eigenvalues and corresponding eigenvectors of $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value, vector = la.eig(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the $k$ largest eigenvalues and corresponding eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000+0.j,  0.15099607+0.j, -0.27029472+0.j])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.argsort(value)[::-1]\n",
    "value = value[idx]\n",
    "vector = vector[:, idx]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07308967,  0.79087251],\n",
       "       [ 0.33258841,  0.5550276 ],\n",
       "       [ 0.94023553, -0.25780813]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 2\n",
    "X = vector[:, 0:k]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form the matrix Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the matrix $Y$ from $X$, by normalizing each $X's$ rows to have unit length (i.e. $Y_{ij}=X_{ij}/(\\sum_{j}{}X_{ij}^2)^{1/2}$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08459823,  0.91540177],\n",
       "       [ 0.37469853,  0.62530147],\n",
       "       [ 1.37778103, -0.37778103]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = X / np.sum(X, 1)[:, np.newaxis]\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating each row of $Y$ as a point in $\\mathbb{R}^k$, cluster into $k$ clusters using K-means.\n",
    "\n",
    "$\\textbf{kmeans}(y, k, max_iter=10)$\n",
    "* K-Means Clustering\n",
    "* Parameters\n",
    "    * y: array-like, shape (n_samples, k)\n",
    "        * Training vectors, where n_samples is the number of samples and k is the number of clusters.\n",
    "    * k: int\n",
    "        * The number of clusters to form as well as the number of centroids to generate.\n",
    "    * max_iter: int\n",
    "        * The number of iterations taken to get the final clustering.\n",
    "* Returns\n",
    "    * clusters: array-like, shape (n_samples, 1)\n",
    "        * The clusters of which the data point belongs to.\n",
    "    * idx_data: array-like, shape (k, n_features)\n",
    "        * The centroids generated from K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans(y, k, max_iter=10):\n",
    "    idx = np.random.choice(len(y), k, replace=False)\n",
    "    idx_data = y[idx]\n",
    "    for i in range(max_iter):\n",
    "        dist = np.array([inner1d(y-c, y-c) for c in idx_data])\n",
    "        clusters = np.argmin(dist, axis=0)\n",
    "        idx_data = np.array([y[clusters==i].mean(axis=0) for i in range(k)])\n",
    "    return (clusters, idx_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters,data=kmeans(Y, 2, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22964838,  0.77035162],\n",
       "       [ 1.37778103, -0.37778103]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign the original point S to clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, assign the original point $s_i$ to cluster $j$ if row $i$ of the matrix $Y$ was assigned to cluster $j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat=np.concatenate((s, clusters.reshape((3,1))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s=pd.DataFrame(s)\n",
    "s['cluster'] = clusters\n",
    "b=pd.DataFrame(data)\n",
    "cluster=range(len(b))\n",
    "b['cluster']=cluster\n",
    "x=pd.merge(s, b, on='cluster', how='outer')\n",
    "c=x.ix[:,0:s.shape[1]-1] \n",
    "c=np.array(c)\n",
    "d=x.ix[:,s.shape[1]:x.shape[1]]\n",
    "d=np.array(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "In previous part, we set $\\sigma^2=1$ to generate the affinity matrix; the $\\sigma^2$ can be human-specified and also can be chosen by minimizing the distortion. Distortion is the sum of the squared Euclidean distance between each data point to its cluster center. Thus we can search over $\\sigma^2$ by searching for the tightest clusters, i.e., the variance corresponding to the minimum distortion. "
=======
    "In previous part, we set $\\sigma^2=1$ to generate the affinity matrix; the $\\sigma^2$ can be human-specified and also can be chosen by minimizing the distortion. \n",
    "\n",
    "Theorem 2 predicts that the rows of $Y$ will form $k$ \"tight\" clusters on the surface of the $k$-sphere. Thus we can search over $\\sigma^2$ by searching for the tightest clusters. "
>>>>>>> 26c0d7d12ea6d0d7e3e096f7713fb9c87102d9fb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distortion\n",
    "\n",
    "* Suppose for a point $x$, you replace its coordinates by the cluster center $c_{x}$ it belongs to (lossy compression).\n",
    "* Measure it with squared Euclidean distance: $x(d)$ is the $d-th$ feature dimension, $y(x)$ is the cluster ID that $x$ is in.\n",
    "    * $\\sum_{d=1...D}{}[x(d)-c_{y(x)}(d)]^2$\n",
    "* This is the distortion of a single point x. For the whole dataset, the distortion is:\n",
    "    * $\\sum_{x}{}\\sum_{d=1...D}{}[x(d)-c_{y(x)}(d)]^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{dist}(x,y)$\n",
    "* Calculate the distance between two matrices\n",
    "* Parameters\n",
    "    * x: array-like \n",
    "        * One of the vectors.\n",
    "    * y: array-like\n",
    "        * One of the vectors.\n",
    "* Returns\n",
    "    * res: float\n",
    "        * Squared Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(x,y):  \n",
    "    res = np.sum((x-y)**2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{findbestvar}(s,vari,k)$\n",
    "* Find the best variance by minimizing the distortion\n",
    "* Parameters\n",
    "    * s: array-like, shape (n_samples, n_features)\n",
    "        * Training vectors, where n_samples is the number of samples and n_features is the number of features.\n",
    "    * vari: array-like, shape (n_vars)\n",
    "        * An array of possible variances.\n",
    "    * k: int\n",
    "        * The number of clusters\n",
    "* Returns\n",
    "    * vari[bestvar]: float\n",
    "        * The right $\\sigma^2$ with mizimum distortion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findbestvar(s,vari,k):                  \n",
    "    n = np.shape(s)[0]\n",
    "    dis=[]\n",
    "    for j in range(len(vari)):\n",
    "        var=vari[j]\n",
    "        A=affinity(s,var)\n",
    "        D = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            D[i, i] = 1 / (A[i].sum())\n",
    "\n",
    "        L = np.sqrt(D).dot(A).dot(np.sqrt(D))\n",
    "\n",
    "        value, vector = la.eig(L)\n",
    "\n",
    "        idx = np.argsort(value)[::-1]\n",
    "        value = value[idx]\n",
    "        vector = vector[:, idx]\n",
    "\n",
    "        X = vector[:, 0:k]\n",
    "        Y = X / np.sum(X, 1)[:, np.newaxis]\n",
    "\n",
    "        clusters,data=kmeans(Y, k, max_iter=100)\n",
    "\n",
    "        snew=pd.DataFrame(s)\n",
    "        snew['cluster'] = clusters\n",
    "        b=pd.DataFrame(data)\n",
    "        cluster=list(unique_everseen(clusters))\n",
    "        b['cluster']=cluster\n",
    "        x=pd.merge(snew, b, on='cluster', how='outer')\n",
    "        c=x.ix[:,0:snew.shape[1]-1] \n",
    "        c=np.array(c)\n",
    "        d=x.ix[:,snew.shape[1]:x.shape[1]]\n",
    "        d=np.array(d)\n",
    "        distance=dist(c,d)\n",
    "\n",
    "        dis.append(distance)\n",
    "    \n",
    "    bestvar=np.argmin(dis)\n",
    "    return(vari[bestvar])\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose the best variance, we set the variances be 100 evenly spaced numbers from 0.1 to 10 and find the best variance using $findbestvar(s,vari,k)$ function."
   ]
  },
  {
=======
>>>>>>> 26c0d7d12ea6d0d7e3e096f7713fb9c87102d9fb
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = np.array([[2,1], [3,4], [5,4]]) \n",
    "vari= np.linspace(0.1, 10, 100)\n",
    "k = 2\n",
    "bestvariance=findbestvar(s,vari,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestvariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final clustering on S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Assign the original point $s_i$ to cluster $j$ generated by K-means clustering. In the creation of the affinity matrix, use the best variance $\\sigma^2$ we found from previous part."
=======
    "Assign the original point $s_i$ to cluster $j$ generated by K-means clustering. "
>>>>>>> 26c0d7d12ea6d0d7e3e096f7713fb9c87102d9fb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{finalclustering}(s,var,k)$\n",
    "* Final clustering on the datapoints\n",
    "* Parameters\n",
    "    * s: array-like, shape (n_samples, n_features)\n",
    "        * Training vectors, where n_samples is the number of samples and n_features is the number of features.\n",
    "    * var: float\n",
    "        * The best variances.\n",
    "    * k: int\n",
    "        * The number of clusters.\n",
    "* Returns\n",
    "    * final: array-like, shape (n_samples, n_features+1)\n",
    "        * The original set of datapoints with the clusters. \n",
    "    * data: array-like, shape (k, n_features)\n",
    "        * The centroids of K-means clustering.\n",
    "    * clusters: array-like, shape (n_samples)\n",
    "        * The clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finalclustering(s,k,var):\n",
    "    n = np.shape(s)[0]\n",
    "    A=affinity(s,var)\n",
    "    D = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        D[i, i] = 1 / (A[i].sum())\n",
    "\n",
    "    L = np.sqrt(D).dot(A).dot(np.sqrt(D))\n",
    "\n",
    "    value, vector = la.eig(L)\n",
    "\n",
    "    idx = np.argsort(value)[::-1]\n",
    "    value = value[idx]\n",
    "    vector = vector[:, idx]\n",
    "\n",
    "    X = vector[:, 0:k]\n",
    "    Y = X / np.sum(X, 1)[:, np.newaxis]\n",
    "\n",
    "    clusters,data=kmeans(Y, k, max_iter=10)\n",
    "    final=np.concatenate((s, clusters.reshape((len(clusters),1))), axis = 1)\n",
    "    return(final,data,clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1]\n",
      " [3 4 1]\n",
      " [5 4 0]]\n",
      "[[ 1.25609126 -0.25609126]\n",
      " [ 0.18188959  0.81811041]]\n",
      "[1 1 0]\n"
     ]
    }
   ],
   "source": [
    "finalclustering(s,k,var=bestvariance)\n",
    "\n",
    "x,y,z=finalclustering(s,k,var=bestvariance)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test algorithm on \"ideal\" datasets"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part, we illustrated the algorithm step by step on a simple data set with 3 points and 2 features. To better understand the application of the algorithm, we can consider its performance on the \"ideal\" dataset in which all points in different clusters are far apart from each other. Here we generated isotropic Gaussian blobs of sample size 50 with two centers [5,5] and [10,10] for clustering. Then find the best variance for this case, and use $finalclustering(s,k,var=bestvariance)$ to cluster the data points into 2 clusters. The clustered points are shown in the figure below. According to the figure, under the \"ideal\" case where the data points in different clusters are far apart, our algorithm behaves well on clustering. "
   ]
  },
  {
=======
>>>>>>> 26c0d7d12ea6d0d7e3e096f7713fb9c87102d9fb
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
=======
    "#num = 50\n",
    "#s = np.random.multivariate_normal([7,9], [[3, 5],[3, 5]], 25)\n",
    "#s2 = np.random.multivariate_normal([15,19], [[3, 5],[3, 5]], 25)\n",
    "#s = np.array([[2,1], [3,4], [5,4]])\n",
    "#s=np.concatenate((s, s2), axis=0)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "\n",
>>>>>>> 26c0d7d12ea6d0d7e3e096f7713fb9c87102d9fb
    "batch_size = 50\n",
    "centers = [[5, 5], [10, 10]]\n",
    "n_clusters = len(centers)\n",
    "X, labels_true = make_blobs(n_samples=50, centers=centers, cluster_std=0.7)\n",
    "\n",
<<<<<<< HEAD
    "s=X"
=======
    "s=X\n"
>>>>>>> 26c0d7d12ea6d0d7e3e096f7713fb9c87102d9fb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vari= np.linspace(0.01, 10, 100)\n",
    "k = 2\n",
    "bestvariance=findbestvar(s,vari,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final,centers,clusters=finalclustering(s,k,var=bestvariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
<<<<<<< HEAD
    "collapsed": false,
    "scrolled": true
=======
    "collapsed": false
>>>>>>> 26c0d7d12ea6d0d7e3e096f7713fb9c87102d9fb
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c8cdf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(final[:,0],final[:,1],c=final[:,2],cmap=cm.rainbow)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Ng, Andrew Y., Michael I. Jordan, and Yair Weiss. \"On spectral clustering: Analysis and an algorithm.\" Advances in neural information processing systems 2 (2002): 849-856."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
